{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df711aae-4201-4809-893d-21b943482fcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 感情分析に基づく分類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910526c-e937-428f-912e-27b6ba4a8da5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## パッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33268db-40e8-4f40-9ce8-4dd1dc6cdc5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install oseti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a4384-7fe2-424b-b924-d53a2612c439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce3cd4-db06-42a4-acc2-b0f283cb10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oseti\n",
    "\n",
    "analyzer = oseti.Analyzer()\n",
    "print(analyzer.analyze_detail('天国で待ってる。'))\n",
    "print(analyzer.analyze_detail('遅刻したけど楽しかったし嬉しかった。すごく充実した！'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b32f4-7685-4a4e-91b0-03e0513399ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import oseti\n",
    "\n",
    "filename = 'text/kageotoko.corpus.txt'\n",
    "analyzer = oseti.Analyzer()\n",
    "\n",
    "with open(filename, 'r', encoding='UTF-8') as f:\n",
    "    hist = Counter(\n",
    "        element['score']\n",
    "        for line in f\n",
    "        for element in analyzer.analyze_detail(line))\n",
    "\n",
    "print('score count')\n",
    "print('-----------')\n",
    "for k in sorted(hist.keys()):\n",
    "    print('{:>5.02f} {}'.format(k, hist[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584520f-af71-43e8-a5ea-c16d6367e562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# トピックモデルによるクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a32079-cc5f-4713-bef8-15ab03ce0825",
   "metadata": {
    "tags": []
   },
   "source": [
    "## パッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efac54-ee79-4083-a3f5-cb7ae3efdf85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gensim pandas openpyxl\n",
    "!pip install ginza ja-ginza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc90c4-72ad-47d4-af84-fce83ff9168f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 解析対象データをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e8173-2ec4-49b4-993c-81372fdb89a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p sisyou_db\n",
    "\n",
    "for year in ['h18', 'h19', 'h20', 'h21', 'h22', 'h23', 'h24']:\n",
    "    for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        !wget -P sisyou_db -c https://anzeninfo.mhlw.go.jp/anzen/shisyo_xls/sisyou_db_{year}_{month}.xls\n",
    "\n",
    "for year in ['h25', 'h26', 'h27', 'h28', 'h29']:\n",
    "    for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "        !wget -P sisyou_db -c https://anzeninfo.mhlw.go.jp/anzen/shisyo_xls/sisyou_db_{year}_{month}.xlsx\n",
    "\n",
    "## サンプルコードでは平成29年1月分のデータしか利用しませんが、\n",
    "## データとしては平成18年〜平成29年の12年分のデータをダウンロードします。\n",
    "## ぜひご自身でも分析してみてください、"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b862250-e8e0-43e0-9559-2680997caff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1bc80-b35b-4f60-9eac-19ced84a0345",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df=pandas.read_excel('sisyou_db/sisyou_db_h29_01.xlsx')\n",
    "print(df['災害状況'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52165fcc-5ff3-4fb7-b8f0-025de32f662c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import spacy\n",
    "import pandas as pd \n",
    "\n",
    "# 1. 学習中の状況を出力するフォーマットの指定\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "# 2. 解析器の初期化\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "# 3. ドキュメントを読み込んで単語分割する\n",
    "df=pd.read_excel('sisyou_db/sisyou_db_h29_01.xlsx')\n",
    "docs=[]\n",
    "for text in df['災害状況']:\n",
    "    if type(text) != str: continue\n",
    "    doc=[token.lemma_ for token in nlp(text)]\n",
    "    docs.append(doc)\n",
    "\n",
    "#\n",
    "# 4. 極端に頻度が低い語や高い語を除外する\n",
    "#\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 4a. 辞書の作成\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# 4b. 出現が20文書に満たない単語と50%以上の文書に出現する単語を極端とみなして除外する\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# 4c. 上記のように定められた語彙で、文書単語行列(Bag-of-words表現)を求める\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print(f'Number of unique tokens: {len(dictionary)}')\n",
    "print(f'Number of documents: {len(corpus)}')\n",
    "\n",
    "#\n",
    "# 5. バイグラムの計算\n",
    "#\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# 5a. 文書単語行列にバイグラムを加える (出現頻度20以上のもの)\n",
    "phrase_model = Phrases(docs, min_count=20)\n",
    "for idx, doc in enumerate(docs):\n",
    "    # _が含まれるトークンはバイグラム(あるいはそれ以上)なので、追加する\n",
    "    bigram_tokens = [token for token in phrase_model[doc] if '_' in token]\n",
    "    docs[idx].extend(bigram_tokens)\n",
    "\n",
    "#            \n",
    "# 6. LDAモデル\n",
    "#\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "temp = dictionary[0]  # 辞書をメモリに読み込むための処理\n",
    "\n",
    "# 6a. LDAモデルの計算\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    chunksize=2000,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=num_topics,\n",
    "    passes=20,\n",
    ")\n",
    "\n",
    "# 6b. LDAモデルによって得られるトピックの抽出\n",
    "top_topics = model.top_topics(corpus) \n",
    "coherences = [coherence for topic_words, coherence in top_topics]\n",
    "\n",
    "# 6c. 評価指標コヒーレンスを求める\n",
    "avg_topic_coherence = sum(coherences) / num_topics\n",
    "print(f'Average topic coherence: {avg_topic_coherence:.4f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
